{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMiSBeDnUGHMMafa/iR3v+Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shounakk05/Hands-On-ML-Journey/blob/main/Chapter-02/Exercise_04.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now in this exercise I will try to write a full pipeline with a custom transformer to process and predict the data."
      ],
      "metadata": {
        "id": "ODhMqYWgyBYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Step 01: Fatch the data\n",
        "housing = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml2/master/datasets/housing/housing.csv\")\n",
        "\n",
        "# Step 02: Splitting the data using train_test_split\n",
        "train_set, test_set = train_test_split(housing, test_size = 0.2, random_state = 42)\n",
        "\n",
        "# Step 03: Seperating the Attributes and the label\n",
        "housing = train_set.drop(\"median_house_value\", axis = 1)\n",
        "housing_labels = train_set[\"median_house_value\"].copy()\n",
        "\n",
        "# Step 04: Finding the indexes of total_rooms, total_bedrooms, population, households\n",
        "# reason: after functions like SimpleImputer the dataframe will change into numy array, making it impossible to access columns using names\n",
        "rooms_ix, bedrooms_ix, population_ix, households_ix = [\n",
        "    housing.columns.get_loc(c) for c in [\"total_rooms\", \"total_bedrooms\", \"population\", \"households\"]\n",
        "]\n",
        "\n",
        "# Step 05: Creation of my CustomTransformer for creation of additional columns\n",
        "class CombinedAttributeAdder(BaseEstimator, TransformerMixin):\n",
        "  def __init__(self, add_bedrooms_per_room = True):\n",
        "    self.add_bedrooms_per_room = add_bedrooms_per_room\n",
        "  def fit(self, X, y = None):\n",
        "    return self\n",
        "  def transform(self, X):\n",
        "    rooms_per_household = X[:, rooms_ix] / X[:, households_ix]\n",
        "    population_per_household = X[:, population_ix] / X[:, households_ix]\n",
        "\n",
        "    if self.add_bedrooms_per_room:\n",
        "      bedrooms_per_room = X[:, bedrooms_ix] / X[:, rooms_ix]\n",
        "      return np.c_[X, bedrooms_per_room, rooms_per_household, population_per_household]\n",
        "    else:\n",
        "      return np.c_[X, rooms_per_household, population_per_household]\n",
        "\n",
        "# Step 06: Seperating the numerical and catagorical columns for processing\n",
        "num_attr = housing.select_dtypes(include=np.number).columns.tolist()  # Got an error here\n",
        "cat_attr = housing.select_dtypes(include='object').columns.tolist()   # -> my num attribute was giving the column transformer a dataframe instead of columns names list\n",
        "\n",
        "# Step 07: Creation of a numerical_pipeline for processing of numerical columns\n",
        "num_pipeline = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy = \"median\")),\n",
        "    (\"attr_adder\", CombinedAttributeAdder()),\n",
        "    (\"std_scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Step 08: Creation of processing_pipeline for both num. and cat. attr.\n",
        "processing_pipeline = ColumnTransformer(transformers = [\n",
        "    (\"num\", num_pipeline, num_attr),\n",
        "    (\"cat\", OneHotEncoder(), cat_attr)\n",
        "])\n",
        "\n",
        "# Step 09: Creation of full_pipeline for processing both processing_pipeline and RandomForestRegressor and GridSearchCV\n",
        "full_pipeline = Pipeline([\n",
        "    (\"processing\", processing_pipeline),\n",
        "    (\"model\", RandomForestRegressor(n_estimators = 100, random_state = 42))\n",
        "])\n",
        "\n",
        "# Step 10: Starting the pipeline on training data\n",
        "full_pipeline.fit(housing, housing_labels)\n",
        "\n",
        "# Step 11: Test set features and label splitting\n",
        "X_test = test_set.drop(\"median_house_value\", axis = 1)\n",
        "y_test = test_set[\"median_house_value\"].copy()\n",
        "\n",
        "# Step 12: Final predictions on test set\n",
        "y_pred = full_pipeline.predict(X_test)\n",
        "\n",
        "# Step 13: RMSE for performance evaluation\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "print(rmse)"
      ],
      "metadata": {
        "id": "lOkmx3fayVa7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc27c1c7-3ae2-4d7e-eb80-a180e956fbf5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50339.472725301675\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4uXgusUD6Vk6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}